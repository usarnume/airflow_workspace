[2023-10-03T08:05:04.668+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T08:05:04.757+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T08:05:04.758+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 1
[2023-10-03T08:05:04.970+0000] {taskinstance.py:1350} INFO - Executing <Task(HttpSensor): call_http_status_space_devs_api> on 2023-09-29 00:00:00+00:00
[2023-10-03T08:05:04.988+0000] {standard_task_runner.py:57} INFO - Started process 880 to run task
[2023-10-03T08:05:05.007+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'capstone_project', 'call_http_status_space_devs_api', 'scheduled__2023-09-29T00:00:00+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/capstone_project.py', '--cfg-path', '/tmp/tmprjqpydnm']
[2023-10-03T08:05:05.011+0000] {standard_task_runner.py:85} INFO - Job 15: Subtask call_http_status_space_devs_api
[2023-10-03T08:05:05.375+0000] {task_command.py:410} INFO - Running <TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [running]> on host 9a30478b5169
[2023-10-03T08:05:05.805+0000] {taskinstance.py:1570} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='capstone_project' AIRFLOW_CTX_TASK_ID='call_http_status_space_devs_api' AIRFLOW_CTX_EXECUTION_DATE='2023-09-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-29T00:00:00+00:00'
[2023-10-03T08:05:05.811+0000] {http.py:122} INFO - Poking: 
[2023-10-03T08:05:05.888+0000] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/sensors/base.py", line 225, in execute
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/sensors/base.py", line 212, in execute
    poke_return = self.poke(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/http/sensors/http.py", line 137, in poke
    raise exc
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/http/sensors/http.py", line 128, in poke
    extra_options=self.extra_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/http/hooks/http.py", line 148, in run
    session = self.get_conn(headers)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/http/hooks/http.py", line 100, in get_conn
    conn = self.get_connection(self.http_conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 434, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `http_default` isn't defined
[2023-10-03T08:05:05.956+0000] {taskinstance.py:1373} INFO - Marking task as FAILED. dag_id=capstone_project, task_id=call_http_status_space_devs_api, execution_date=20230929T000000, start_date=20231003T080504, end_date=20231003T080505
[2023-10-03T08:05:06.145+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 15 for task call_http_status_space_devs_api (The conn_id `http_default` isn't defined; 880)
[2023-10-03T08:05:06.240+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-10-03T08:05:06.341+0000] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-10-03T09:06:22.205+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T09:06:22.539+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T09:06:22.546+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 1
[2023-10-03T09:06:22.729+0000] {taskinstance.py:1350} INFO - Executing <Task(HttpSensor): call_http_status_space_devs_api> on 2023-09-29 00:00:00+00:00
[2023-10-03T09:06:22.766+0000] {standard_task_runner.py:57} INFO - Started process 540 to run task
[2023-10-03T09:06:22.778+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'capstone_project', 'call_http_status_space_devs_api', 'scheduled__2023-09-29T00:00:00+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/capstone_project.py', '--cfg-path', '/tmp/tmpjnv16hbd']
[2023-10-03T09:06:22.787+0000] {standard_task_runner.py:85} INFO - Job 91: Subtask call_http_status_space_devs_api
[2023-10-03T09:06:23.010+0000] {task_command.py:410} INFO - Running <TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [running]> on host 9a30478b5169
[2023-10-03T09:06:23.556+0000] {taskinstance.py:1570} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='capstone_project' AIRFLOW_CTX_TASK_ID='call_http_status_space_devs_api' AIRFLOW_CTX_EXECUTION_DATE='2023-09-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-29T00:00:00+00:00'
[2023-10-03T09:06:23.557+0000] {http.py:122} INFO - Poking: 
[2023-10-03T09:06:23.612+0000] {base.py:73} INFO - Using connection ID 'thespacedevs_dev' for task execution.
[2023-10-03T09:06:24.535+0000] {base.py:255} INFO - Success criteria met. Exiting.
[2023-10-03T09:06:24.558+0000] {taskinstance.py:1373} INFO - Marking task as SUCCESS. dag_id=capstone_project, task_id=call_http_status_space_devs_api, execution_date=20230929T000000, start_date=20231003T090622, end_date=20231003T090624
[2023-10-03T09:06:24.630+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-10-03T09:06:24.798+0000] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-10-03T09:40:05.536+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T09:40:05.601+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T09:40:05.603+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 1
[2023-10-03T09:40:05.742+0000] {taskinstance.py:1350} INFO - Executing <Task(HttpSensor): call_http_status_space_devs_api> on 2023-09-29 00:00:00+00:00
[2023-10-03T09:40:05.767+0000] {standard_task_runner.py:57} INFO - Started process 1163 to run task
[2023-10-03T09:40:05.797+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'capstone_project', 'call_http_status_space_devs_api', 'scheduled__2023-09-29T00:00:00+00:00', '--job-id', '110', '--raw', '--subdir', 'DAGS_FOLDER/capstone_project.py', '--cfg-path', '/tmp/tmprv9rup1d']
[2023-10-03T09:40:05.811+0000] {standard_task_runner.py:85} INFO - Job 110: Subtask call_http_status_space_devs_api
[2023-10-03T09:40:06.296+0000] {task_command.py:410} INFO - Running <TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [running]> on host 9a30478b5169
[2023-10-03T09:40:07.265+0000] {taskinstance.py:1570} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='capstone_project' AIRFLOW_CTX_TASK_ID='call_http_status_space_devs_api' AIRFLOW_CTX_EXECUTION_DATE='2023-09-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-29T00:00:00+00:00'
[2023-10-03T09:40:07.266+0000] {http.py:122} INFO - Poking: 
[2023-10-03T09:40:07.323+0000] {base.py:73} INFO - Using connection ID 'thespacedevs_dev' for task execution.
[2023-10-03T09:40:07.905+0000] {base.py:255} INFO - Success criteria met. Exiting.
[2023-10-03T09:40:07.925+0000] {taskinstance.py:1373} INFO - Marking task as SUCCESS. dag_id=capstone_project, task_id=call_http_status_space_devs_api, execution_date=20230929T000000, start_date=20231003T094005, end_date=20231003T094007
[2023-10-03T09:40:07.986+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-10-03T09:40:08.339+0000] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-10-03T12:33:28.162+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T12:33:28.399+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T12:33:28.418+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 1
[2023-10-03T12:33:28.770+0000] {taskinstance.py:1350} INFO - Executing <Task(HttpSensor): call_http_status_space_devs_api> on 2023-09-29 00:00:00+00:00
[2023-10-03T12:33:29.005+0000] {standard_task_runner.py:57} INFO - Started process 99 to run task
[2023-10-03T12:33:29.077+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'capstone_project', 'call_http_status_space_devs_api', 'scheduled__2023-09-29T00:00:00+00:00', '--job-id', '226', '--raw', '--subdir', 'DAGS_FOLDER/capstone_project.py', '--cfg-path', '/tmp/tmprgps8qbr']
[2023-10-03T12:33:29.124+0000] {standard_task_runner.py:85} INFO - Job 226: Subtask call_http_status_space_devs_api
[2023-10-03T12:33:30.219+0000] {task_command.py:410} INFO - Running <TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [running]> on host 9a30478b5169
[2023-10-03T12:33:31.388+0000] {local_task_job_runner.py:122} ERROR - Received SIGTERM. Terminating subprocesses
[2023-10-03T12:33:31.391+0000] {process_utils.py:135} INFO - Sending Signals.SIGTERM to group 99. PIDs of all processes in the group: [99]
[2023-10-03T12:33:31.391+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 99
[2023-10-03T12:33:31.402+0000] {taskinstance.py:1540} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-10-03T12:33:31.542+0000] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1430, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1557, in _execute_task_with_callbacks
    RenderedTaskInstanceFields.write(rtif)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/renderedtifields.py", line 183, in write
    session.merge(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 3059, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 3137, in _merge
    options=options,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2858, in get
    execution_options=execution_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2976, in _get_impl
    load_options=load_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 534, in load_on_pk_identity
    bind_arguments=bind_arguments,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1714, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 335, in _execute_on_connection
    self, multiparams, params, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1570, in _execute_clauseelement
    linting=self.dialect.compiler_linting | compiler.WARN_LINTING,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 538, in _compile_w_cache
    **kw
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 567, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/compiler.py", line 809, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/compiler.py", line 464, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/compiler.py", line 499, in process
    return obj._compiler_dispatch(self, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/visitors.py", line 82, in _compiler_dispatch
    return meth(self, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/compiler.py", line 3406, in visit_select
    select_stmt, self, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/base.py", line 510, in create_for_statement
    return klass.create_for_statement(statement, compiler, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/context.py", line 765, in create_for_statement
    self._setup_for_generate()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/context.py", line 875, in _setup_for_generate
    entity.setup_compile_state(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/context.py", line 2670, in setup_compile_state
    polymorphic_discriminator=self._polymorphic_discriminator,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 621, in _setup_entity_query
    **kw
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/interfaces.py", line 647, in setup
    context, query_entity, path, loader, adapter, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/strategies.py", line 2039, in setup_query
    chained_from_outerjoin=chained_from_outerjoin,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py", line 621, in _setup_entity_query
    **kw
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/interfaces.py", line 647, in setup
    context, query_entity, path, loader, adapter, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/strategies.py", line 208, in setup_query
    c = adapter.columns[c]
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/util.py", line 991, in __getitem__
    return self.columns[key]
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/_collections.py", line 762, in __missing__
    self[key] = val = self.creator(self.weakself(), key)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/util.py", line 1030, in _locate_col
    c = vis.replace(col, _include_singleton_constants=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/util.py", line 914, in replace
    return self._corresponding_column(col, True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/util.py", line 840, in _corresponding_column
    col, require_embedded=require_embedded
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/selectable.py", line 227, in corresponding_column
    column, require_embedded
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/base.py", line 1399, in corresponding_column
    expanded_proxy_set = set(_expand_cloned(c.proxy_set))
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1542, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-10-03T12:33:31.846+0000] {taskinstance.py:1373} INFO - Marking task as FAILED. dag_id=capstone_project, task_id=call_http_status_space_devs_api, execution_date=20230929T000000, start_date=20231003T123328, end_date=20231003T123331
[2023-10-03T12:33:32.121+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 226 for task call_http_status_space_devs_api (Task received SIGTERM signal; 99)
[2023-10-03T12:33:32.199+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=99, status='terminated', exitcode=1, started='12:33:28') (99) terminated with exit code 1
[2023-10-03T12:33:32.199+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 143
[2023-10-03T12:33:32.866+0000] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-10-03T12:56:58.701+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T12:56:58.874+0000] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [queued]>
[2023-10-03T12:56:58.875+0000] {taskinstance.py:1331} INFO - Starting attempt 1 of 1
[2023-10-03T12:56:59.227+0000] {taskinstance.py:1350} INFO - Executing <Task(HttpSensor): call_http_status_space_devs_api> on 2023-09-29 00:00:00+00:00
[2023-10-03T12:56:59.377+0000] {standard_task_runner.py:57} INFO - Started process 124 to run task
[2023-10-03T12:56:59.435+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'capstone_project', 'call_http_status_space_devs_api', 'scheduled__2023-09-29T00:00:00+00:00', '--job-id', '298', '--raw', '--subdir', 'DAGS_FOLDER/capstone_project.py', '--cfg-path', '/tmp/tmpv9xd0_xi']
[2023-10-03T12:56:59.520+0000] {standard_task_runner.py:85} INFO - Job 298: Subtask call_http_status_space_devs_api
[2023-10-03T12:57:00.083+0000] {task_command.py:410} INFO - Running <TaskInstance: capstone_project.call_http_status_space_devs_api scheduled__2023-09-29T00:00:00+00:00 [running]> on host 9a30478b5169
[2023-10-03T12:57:02.259+0000] {taskinstance.py:1570} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='capstone_project' AIRFLOW_CTX_TASK_ID='call_http_status_space_devs_api' AIRFLOW_CTX_EXECUTION_DATE='2023-09-29T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-29T00:00:00+00:00'
[2023-10-03T12:57:02.275+0000] {http.py:122} INFO - Poking: 
[2023-10-03T12:57:02.692+0000] {base.py:73} INFO - Using connection ID 'thespacedevs_dev' for task execution.
[2023-10-03T12:57:03.761+0000] {base.py:255} INFO - Success criteria met. Exiting.
[2023-10-03T12:57:03.874+0000] {taskinstance.py:1373} INFO - Marking task as SUCCESS. dag_id=capstone_project, task_id=call_http_status_space_devs_api, execution_date=20230929T000000, start_date=20231003T125658, end_date=20231003T125703
[2023-10-03T12:57:04.777+0000] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-10-03T12:57:06.248+0000] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
